# AgenticWork Chat API - Environment Configuration Template
# Copy this file to .env and configure all values - NO DEFAULTS ALLOWED

# === MCP Proxy Configuration ===
MCP_PROXY_URL=http://mcp-proxy:3100
MCP_PROXY_HOST=mcp-proxy
MCP_PROXY_PORT=3100

# === LLM Provider Configuration ===
# Primary LLM Provider: vertex-ai | azure-openai | aws-bedrock
DEFAULT_LLM_PROVIDER=vertex-ai

# === Google Vertex AI Configuration (PRIMARY) ===
VERTEX_AI_ENABLED=true
VERTEX_AI_PRIORITY=1
GCP_PROJECT_ID=agenticwork-dev
GCP_LOCATION=us-central1
# Service account JSON (base64 encoded or file path)
GCP_SERVICE_ACCOUNT_JSON_BASE64=
# Or provide path to service account JSON file
GCP_SERVICE_ACCOUNT_JSON_PATH=
# Model configuration - Gemini models
VERTEX_AI_MODEL_ID=gemini-2.0-flash-001
VERTEX_AI_EMBEDDING_MODEL=text-embedding-004

# === Vertex AI Claude Models (via Model Garden) ===
# Claude Sonnet 4.5 for complex reasoning tasks
VERTEX_CLAUDE_MODEL_ID=publishers/anthropic/models/claude-sonnet-4-5
VERTEX_CLAUDE_VERSION=claude-sonnet-4-5@20250929
# Use Claude for: expert reasoning, complex analysis, nuanced tasks
VERTEX_CLAUDE_ENABLED=true

# === Ollama Configuration (LOCAL INFERENCE) ===
OLLAMA_ENABLED=true
OLLAMA_BASE_URL=http://ollama:11434
OLLAMA_PRIORITY=10
# Default model for simple queries (fast, local, no cost)
OLLAMA_MODEL=gpt-oss
# Embedding model for semantic search
OLLAMA_EMBEDDING_MODEL=embeddinggemma
# Health check model
OLLAMA_HEALTH_MODEL=qwen2.5-coder:7b

# === Intelligent Routing Configuration ===
# Route simple queries to Ollama (local, fast, free)
ROUTE_SIMPLE_TO_OLLAMA=true
# Route complex reasoning to Claude Sonnet 4.5
ROUTE_COMPLEX_TO_CLAUDE=true
# Complexity threshold for Ollama (simple queries only)
OLLAMA_MAX_COMPLEXITY=simple
# Fall back to Gemini if Ollama/Claude unavailable
FALLBACK_MODEL=gemini-2.0-flash-001

# === Azure OpenAI Configuration (FALLBACK - Priority 2) ===
AZURE_OPENAI_ENABLED=false
AZURE_OPENAI_PRIORITY=2
DEFAULT_MODEL_PREFIX=azure
AZURE_OPENAI_EMBEDDING_DEPLOYMENT=
DEFAULT_EMBEDDING_DEPLOYMENT=
EMBEDDING_MODEL=
DEFAULT_EMBEDDING_MODEL=
VISION_EMBEDDING_MODEL=
AZURE_OPENAI_VISION_DEPLOYMENT=
VISION_MODEL=

# === Database Configuration (REQUIRED) ===
POSTGRES_HOST=
POSTGRES_PORT=
POSTGRES_USER=
POSTGRES_PASSWORD=
DATABASE_URL=

# === Milvus Vector Database (REQUIRED) ===
MILVUS_HOST=
MILVUS_PORT=
MILVUS_USER=
MILVUS_PASSWORD=

# === Token Limits (REQUIRED) ===
AZURE_OPENAI_DAILY_TOKEN_LIMIT=
DEFAULT_DAILY_TOKEN_LIMIT=
AZURE_OPENAI_MONTHLY_TOKEN_LIMIT=
DEFAULT_MONTHLY_TOKEN_LIMIT=
AZURE_OPENAI_MAX_TOKENS_PER_REQUEST=
DEFAULT_MAX_TOKENS_PER_REQUEST=

# === Vector Embedding Configuration (REQUIRED) ===
EMBEDDING_DIMENSION=1536

# === Application Configuration ===
LOG_LEVEL=info
NODE_ENV=production

# === Cache Configuration ===
ENABLE_RAG_CACHE=true
RAG_CACHE_TTL=300
# === Azure AI Foundry Metrics (OPTIONAL) ===
# Enable per-model metrics collection from Azure AI Foundry using Entra ID authentication
# AZURE_SUBSCRIPTION_ID=your-azure-subscription-id
# AZURE_RESOURCE_GROUP=your-resource-group-name
# AZURE_OPENAI_ACCOUNT_NAME=your-azure-openai-account-name
# AIF_METRICS_TIME_RANGE_MINUTES=60  # Metrics collection window (default: 60)
# AIF_METRICS_REFRESH_INTERVAL_MINUTES=5  # How often to refresh metrics (default: 5)

# === MCP Call Logging (OPTIONAL) ===
# MCP Proxy will send call logs to API for storage and analytics
# API_BASE_URL is configured in mcp-proxy service, not needed here

# === Phase 3: Cross-Encoder Reranking (OPTIONAL) ===
# Enable LLM-based reranking for improved search accuracy
# Reranks top-20 results using LLM to ensure best tools are surfaced
# Note: Adds latency (~500-1000ms) and LLM token cost
ENABLE_RERANKING=false

# === Tiered Function Calling Configuration ===
# Configurable models for function calling based on intelligence slider
# Empty = use default model selection, Set = use specific model for tier
#
# CHEAP TIER (Slider 0-40%): Fast, low-cost models for simple function calls
# Recommended: gemini-2.0-flash-001, gpt-4o-mini, llama3.2
FUNCTION_CALLING_MODEL_CHEAP=
#
# BALANCED TIER (Slider 41-60%): Good accuracy, moderate cost
# Recommended: gemini-2.0-flash-001, gpt-4o
FUNCTION_CALLING_MODEL_BALANCED=
#
# PREMIUM TIER (Slider 61-100%): Best accuracy, higher cost
# Recommended: claude-sonnet-4-5, gpt-4o, gemini-2.5-pro
FUNCTION_CALLING_MODEL_PREMIUM=
#
# Tool Stripping: Strip tools from requests when message doesn't need them
# Saves ~2000+ tokens per request for pure chat messages
TOOL_STRIPPING_ENABLED=true
#
# Function Call Decision Caching: Cache routing decisions for similar messages
# Reduces repeated analysis overhead
FUNCTION_DECISION_CACHE_ENABLED=true
FUNCTION_DECISION_CACHE_TTL_SECONDS=300

# === Multi-Model Collaboration Configuration ===
# Enable multi-model orchestration where different models handle different roles
# Reasoning model → Tool execution model → Synthesis model
# Set to true to enable, false to use single-model mode (default)
ENABLE_MULTI_MODEL=false

# ===== REQUIRED: Primary Models for Each Role =====
# These MUST be set when ENABLE_MULTI_MODEL=true
# Use model IDs from your configured providers (e.g., gemini-2.0-flash-001, claude-sonnet-4)
MULTI_MODEL_REASONING_PRIMARY=
MULTI_MODEL_TOOL_PRIMARY=
MULTI_MODEL_SYNTHESIS_PRIMARY=
MULTI_MODEL_FALLBACK_PRIMARY=

# ===== OPTIONAL: Fallback Models for Each Role =====
# Used when primary model fails
MULTI_MODEL_REASONING_FALLBACK=
MULTI_MODEL_TOOL_FALLBACK=
MULTI_MODEL_SYNTHESIS_FALLBACK=

# ===== OPTIONAL: Tier-Specific Model Overrides =====
# Override models based on slider position (ECONOMY <75, BALANCED 75-89, PREMIUM 90+)
# MULTI_MODEL_ECONOMY_REASONING=
# MULTI_MODEL_ECONOMY_TOOL=
# MULTI_MODEL_ECONOMY_SYNTHESIS=
# MULTI_MODEL_BALANCED_REASONING=
# MULTI_MODEL_BALANCED_TOOL=
# MULTI_MODEL_BALANCED_SYNTHESIS=
# MULTI_MODEL_PREMIUM_REASONING=
# MULTI_MODEL_PREMIUM_TOOL=
# MULTI_MODEL_PREMIUM_SYNTHESIS=
# MULTI_MODEL_PREMIUM_THINKING_BUDGET=16000

# ===== OPTIONAL: Complexity-Based Single Model Selection =====
# When multi-model is not triggered, use these models based on query complexity
# MULTI_MODEL_SIMPLE_MODEL=
# MULTI_MODEL_MODERATE_MODEL=
# MULTI_MODEL_COMPLEX_MODEL=
# MULTI_MODEL_EXPERT_MODEL=

# ===== OPTIONAL: Model Role Settings =====
# MULTI_MODEL_REASONING_TEMP=0.7
# MULTI_MODEL_REASONING_THINKING_BUDGET=8000
# MULTI_MODEL_REASONING_ENABLE_THINKING=true
# MULTI_MODEL_TOOL_TEMP=0.3
# MULTI_MODEL_SYNTHESIS_TEMP=0.5
# MULTI_MODEL_FALLBACK_TEMP=0.5

# ===== OPTIONAL: Cost Tracking per Model =====
# Format: input_cost_per_1M,output_cost_per_1M
# MULTI_MODEL_COST_CLAUDE_OPUS_4=15,75
# MULTI_MODEL_COST_CLAUDE_SONNET_4=3,15
# MULTI_MODEL_COST_GEMINI_2_0_FLASH_001=0.15,0.6

# ===== Routing Configuration =====
# Multi-Model Complexity Threshold (0-100)
# Requests with complexity score above this threshold will trigger multi-model
MULTI_MODEL_COMPLEXITY_THRESHOLD=60

# Maximum handoffs between models in a single request
MULTI_MODEL_MAX_HANDOFFS=5

# Enable auto-fallback to single model on multi-model errors
MULTI_MODEL_AUTO_FALLBACK=true

# Slider position threshold to enable multi-model (0-100)
# Multi-model only activates when slider is at or above this position
MULTI_MODEL_SLIDER_THRESHOLD=70

# Comma-separated patterns that always trigger multi-model
# MULTI_MODEL_TRIGGER_PATTERNS=analyze,compare,audit,comprehensive,investigate

# Whether to prefer cheaper models for tool execution
# MULTI_MODEL_PREFER_CHEAP_TOOLS=true

# Whether to scale model selection by slider position
# MULTI_MODEL_SCALE_BY_SLIDER=true
